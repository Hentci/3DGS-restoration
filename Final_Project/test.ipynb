{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import DDPMPipeline\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from utils import utils_blindsr as blindsr\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft.tuners.lora import LoraModel\n",
    "from tqdm import tqdm\n",
    "from transformers import get_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "torch.Size([1, 3, 256, 256])\n",
      "('',)\n"
     ]
    }
   ],
   "source": [
    "class MipNeRF360Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root='/home/hentci/vimeo_triplet/simple/lq'):\n",
    "        self.images = sorted(os.listdir(root))\n",
    "        self.label = ''\n",
    "        self.root = root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Resize((512, 512)),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "        image = transform(Image.open(os.path.join(self.root, self.images[idx])))\n",
    "        \n",
    "        return image, self.label, self.images[idx]\n",
    "\n",
    "\n",
    "train_dataset = MipNeRF360Dataset()\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "img, label, filenames = next(iter(train_dataloader))\n",
    "print(len(train_dataloader))\n",
    "print(img.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  7.22it/s]\n",
      "Loading pipeline components...: 100%|██████████| 5/5 [00:00<00:00, 16.26it/s]\n"
     ]
    }
   ],
   "source": [
    "pipeline = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16, variant=\"fp16\", use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=pipeline.text_encoder_2,\n",
    "    vae=pipeline.vae,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\",\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:00<00:07,  6.07it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:09<00:00,  5.55it/s]\n"
     ]
    }
   ],
   "source": [
    "image = pipeline(prompt='', output_type=\"latent\").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 20.32it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.42it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.51it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.53it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 18.92it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.66it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.20it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.34it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 27.29it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.72it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.34it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.35it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.26it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.41it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.19it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.44it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.54it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.20it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.57it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.47it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.24it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.03it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.24it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.62it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.58it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.67it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.41it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.75it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.04it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.62it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.34it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.55it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.77it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.40it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.70it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.56it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.50it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.61it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.49it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.36it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.929411768913269]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.49it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.9372549057006836]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.36it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.8980392217636108]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.56it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.36it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.64it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.66it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.39it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.68it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9843137264251709,0.9529411792755127]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.70it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9215686321258545,0.9137254953384399]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.86it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.53it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.98it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.99it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.51it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.77it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.9607843160629272]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.67it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.64it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.77it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.67it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.72it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.31it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.65it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.64it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.74it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.03it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9843137264251709,0.9450980424880981]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 18.30it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.01it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.97it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.82it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9529411792755127,0.9372549057006836]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.77it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.37it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.91it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.25it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.65it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.93it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.929411768913269,0.9764705896377563]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.00it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.04it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.91it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.18it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.04it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.14it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.91it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.929411768913269,0.9372549057006836]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.86it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.80it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.84it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.31it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.37it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.99it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.91it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.86it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.8901960849761963]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.95it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9137254953384399,0.8588235378265381]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.84it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.92it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.84it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9372549057006836,0.9137254953384399]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.76it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.67it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.75it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.50it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.90it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.90it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.76it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.82it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.91it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.87it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.88it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.51it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.94it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.94it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.63it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.84it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.65it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.90it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.75it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.84it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.529411792755127]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.37it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.7176470756530762]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.70it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.5372549295425415]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.37it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.48it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.78it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.78it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.03it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.33it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.77it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.78it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.49it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.60it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.87it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.76it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.86it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.78it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.46it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.25it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.74it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.57it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.7333333492279053]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.94it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.60it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.42it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.47it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.7803921699523926]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.63it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.33it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.5686274766921997]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.47it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.10it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.61it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.843137264251709]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.48it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.92it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.00it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.84it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.50it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.8509804010391235]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.61it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.75it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.37it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.00it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.6627451181411743]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.59it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 19.94it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.02it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 19.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.15it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.70it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.64it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.87it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.88it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.88it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9607843160629272,0.6470588445663452]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.75it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.9529411792755127]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.8039215803146362]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.28it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.32549023628234863]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.75it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 19.85it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.4431372880935669]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.40it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.86it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.90it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.90it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.82it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.35it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.49it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.36it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.50it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.29it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.54it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.58it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.36it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.43it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.49it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.54it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.65it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.8666666746139526]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.19it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.50it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.55it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.41it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.22it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.17it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.06it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.73it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.82it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.96it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.91it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.86it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.90it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.30it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.67it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.58it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.97it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.67it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.83it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.74it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9843137264251709,0.9058823585510254]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.78it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.88it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.01it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.60it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.06it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.92it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.02it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.95it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.05it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.78it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.95it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.02it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.93it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.96it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.95it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.91it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.59it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.77it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.80it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.69it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.67it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.8666666746139526,0.32549023628234863]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.81it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.15it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9764705896377563,0.6313725709915161]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.06it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.89it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9843137264251709,0.529411792755127]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.96it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-1.0,0.8352941274642944]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.85it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.96it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.07it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.93it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.59it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.60it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.79it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.74it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.53it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.78it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.48it/s]\n",
      "/home/hentci/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/image_processor.py:582: FutureWarning: Passing `image` as torch tensor with value range in [-1,1] is deprecated. The expected value range for image tensor is [0,1] when passing as pytorch tensor or numpy Array. You passed `image` with value range [-0.9607843160629272,0.9843137264251709]\n",
      "  warnings.warn(\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.05it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.92it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 21.07it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.96it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.97it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.92it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.99it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.97it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.96it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.58it/s]\n",
      "100%|██████████| 15/15 [00:00<00:00, 20.63it/s]\n"
     ]
    }
   ],
   "source": [
    "result_dir = '/home/hentci/Final_Project/results/refiner/vimeo_lq'\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "for img, label, filename in train_dataloader:\n",
    "    image = refiner(prompt='', image=img[0]).images[0]\n",
    "    image.save(os.path.join(result_dir, filename[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): UNet2DConditionModel(\n",
       "      (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (time_proj): Timesteps()\n",
       "      (time_embedding): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (add_time_proj): Timesteps()\n",
       "      (add_embedding): TimestepEmbedding(\n",
       "        (linear_1): Linear(in_features=2816, out_features=1280, bias=True)\n",
       "        (act): SiLU()\n",
       "        (linear_2): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "      )\n",
       "      (down_blocks): ModuleList(\n",
       "        (0): DownBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0-1): 2 x BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (downsamplers): ModuleList(\n",
       "            (0): Downsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): CrossAttnDownBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-1): 2 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0-9): 10 x BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (up_blocks): ModuleList(\n",
       "        (0): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0-9): 10 x BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0-1): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): CrossAttnUpBlock2D(\n",
       "          (attentions): ModuleList(\n",
       "            (0-2): 3 x Transformer2DModel(\n",
       "              (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "              (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
       "              (transformer_blocks): ModuleList(\n",
       "                (0-1): 2 x BasicTransformerBlock(\n",
       "                  (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn1): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attn2): Attention(\n",
       "                    (to_q): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=640, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=640, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_k): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_v): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=2048, out_features=640, bias=False)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.1, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=4, out_features=640, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (to_out): ModuleList(\n",
       "                      (0): Linear(in_features=640, out_features=640, bias=True)\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "                  (ff): FeedForward(\n",
       "                    (net): ModuleList(\n",
       "                      (0): GEGLU(\n",
       "                        (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
       "                      )\n",
       "                      (1): Dropout(p=0.0, inplace=False)\n",
       "                      (2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "                    )\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (2): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=640, bias=True)\n",
       "              (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "          (upsamplers): ModuleList(\n",
       "            (0): Upsample2D(\n",
       "              (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): UpBlock2D(\n",
       "          (resnets): ModuleList(\n",
       "            (0): ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (1-2): 2 x ResnetBlock2D(\n",
       "              (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "              (conv1): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (time_emb_proj): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "              (nonlinearity): SiLU()\n",
       "              (conv_shortcut): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (mid_block): UNetMidBlock2DCrossAttn(\n",
       "        (attentions): ModuleList(\n",
       "          (0): Transformer2DModel(\n",
       "            (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "            (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (transformer_blocks): ModuleList(\n",
       "              (0-9): 10 x BasicTransformerBlock(\n",
       "                (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn1): Attention(\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (attn2): Attention(\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_k): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_v): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=2048, out_features=1280, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=2048, out_features=4, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=4, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_out): ModuleList(\n",
       "                    (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "                (ff): FeedForward(\n",
       "                  (net): ModuleList(\n",
       "                    (0): GEGLU(\n",
       "                      (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
       "                    )\n",
       "                    (1): Dropout(p=0.0, inplace=False)\n",
       "                    (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (conv1): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (time_emb_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "      (conv_act): SiLU()\n",
       "      (conv_out): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配置 LoRA 参数\n",
    "lora_config = LoraConfig(\n",
    "    r=4,            # 矩阵降维因子\n",
    "    lora_alpha=16,  # LoRA scaling因子\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\"],  # 选择目标模块\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# 获取微调模型\n",
    "lora_model = get_peft_model(pipeline.unet, lora_config)\n",
    "\n",
    "# 设置为训练模式\n",
    "lora_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,474,880 || all params: 2,571,938,564 || trainable%: 0.1740\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 512, 512])\n",
      "('', '', '', '', '', '', '', '')\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.AdamW(lora_model.parameters(), lr=5e-5)\n",
    "\n",
    "# 学习率调度器\n",
    "num_epochs = 3\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_dataloader) * num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 24\u001b[0m\n\u001b[1;32m     17\u001b[0m noisy_x \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39madd_noise(images, noise, timesteps)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# token = pipeline.tokenizer(prompts, padding=True, return_tensors=\"pt\").to('cuda')\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# encoder_hidden_states = pipeline.text_encoder(**token)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# encoder_hidden_states = pipeline.encode_prompt()\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mlora_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(pred, noise)\n",
      "File \u001b[0;32m~/miniconda3/envs/hentci/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hentci/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hentci/lib/python3.10/site-packages/peft/peft_model.py:762\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    761\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hentci/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hentci/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:1152\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1150\u001b[0m         emb \u001b[38;5;241m=\u001b[39m emb \u001b[38;5;241m+\u001b[39m class_emb\n\u001b[0;32m-> 1152\u001b[0m aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_aug_embed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1153\u001b[0m \u001b[43m    \u001b[49m\u001b[43memb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madded_cond_kwargs\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maddition_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_hint\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1156\u001b[0m     aug_emb, hint \u001b[38;5;241m=\u001b[39m aug_emb\n",
      "File \u001b[0;32m~/miniconda3/envs/hentci/lib/python3.10/site-packages/diffusers/models/unets/unet_2d_condition.py:969\u001b[0m, in \u001b[0;36mUNet2DConditionModel.get_aug_embed\u001b[0;34m(self, emb, encoder_hidden_states, added_cond_kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m     aug_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_embedding(text_embs, image_embs)\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maddition_embed_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;66;03m# SDXL - style\u001b[39;00m\n\u001b[0;32m--> 969\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext_embeds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madded_cond_kwargs\u001b[49m:\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has the config param `addition_embed_type` set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_time\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which requires the keyword argument `text_embeds` to be passed in `added_cond_kwargs`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    972\u001b[0m         )\n\u001b[1;32m    973\u001b[0m     text_embeds \u001b[38;5;241m=\u001b[39m added_cond_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "# 将模型移动到 GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lora_model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lora_model.train()\n",
    "    epoch_loss = 0\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    for i, batch in enumerate(pbar):\n",
    "        images, prompts = batch\n",
    "        images = images.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        noise = torch.randn_like(images)\n",
    "        timesteps = torch.randint(0, 999, (images.shape[0],)).long().to('cuda')\n",
    "        noisy_x = pipeline.scheduler.add_noise(images, noise, timesteps)\n",
    "        \n",
    "        # token = pipeline.tokenizer(prompts, padding=True, return_tensors=\"pt\").to('cuda')\n",
    "        # encoder_hidden_states = pipeline.text_encoder(**token)\n",
    "        # encoder_hidden_states = pipeline.encode_prompt()\n",
    "        \n",
    "        # 前向传播\n",
    "        pred = lora_model(noisy_x, timesteps, '')\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = loss_function(pred, noise)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss / len(train_dataloader)}\")\n",
    "\n",
    "# 保存微调后的模型\n",
    "torch.save(lora_model.state_dict(), \"finetuned_lora_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "torch.tensor(pipeline.encode_prompt(list(('', '', '', '', '', '', '', ''))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPooling(last_hidden_state=tensor([[[-3.8843e-01,  2.2949e-02, -5.2338e-02,  ..., -4.9023e-01,\n",
       "          -3.0664e-01,  6.7383e-02],\n",
       "         [ 2.7878e-02, -1.3242e+00,  3.0859e-01,  ..., -5.2539e-01,\n",
       "           9.7461e-01,  6.6406e-01],\n",
       "         [ 1.1572e+00,  1.3306e-01,  7.9004e-01,  ..., -2.1035e+00,\n",
       "          -1.1514e+00, -3.3228e-01],\n",
       "         ...,\n",
       "         [ 5.8447e-01, -1.3806e-01,  2.1562e+00,  ..., -1.0508e+00,\n",
       "          -1.5222e-01,  9.3140e-02],\n",
       "         [-7.8186e-02,  9.8242e-01,  6.9189e-01,  ..., -2.8887e+00,\n",
       "           2.1088e-02, -4.1382e-01],\n",
       "         [-1.2158e+00, -5.1367e-01,  4.8022e-01,  ..., -1.3782e-01,\n",
       "           8.1250e-01,  5.5811e-01]],\n",
       "\n",
       "        [[-3.8843e-01,  2.2949e-02, -5.2338e-02,  ..., -4.9023e-01,\n",
       "          -3.0664e-01,  6.7383e-02],\n",
       "         [ 2.7878e-02, -1.3242e+00,  3.0859e-01,  ..., -5.2539e-01,\n",
       "           9.7461e-01,  6.6406e-01],\n",
       "         [ 1.1572e+00,  1.3306e-01,  7.9004e-01,  ..., -2.1035e+00,\n",
       "          -1.1514e+00, -3.3228e-01],\n",
       "         ...,\n",
       "         [ 5.8447e-01, -1.3806e-01,  2.1562e+00,  ..., -1.0508e+00,\n",
       "          -1.5222e-01,  9.3140e-02],\n",
       "         [-1.8018e+00,  7.1387e-01,  9.7607e-01,  ..., -1.3799e+00,\n",
       "          -9.1455e-01,  1.2764e-02],\n",
       "         [-9.5117e-01,  6.7749e-03,  1.4075e-01,  ...,  3.9983e-04,\n",
       "           2.2217e-01,  8.9111e-01]]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-1.2158e+00, -5.1367e-01,  4.8022e-01,  ..., -1.3782e-01,\n",
       "          8.1250e-01,  5.5811e-01],\n",
       "        [-9.5117e-01,  6.7749e-03,  1.4075e-01,  ...,  3.9983e-04,\n",
       "          2.2217e-01,  8.9111e-01]], device='cuda:0', dtype=torch.float16,\n",
       "       grad_fn=<IndexBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = pipeline.tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\").to('cuda')\n",
    "pipeline.text_encoder(**token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-3.8926, -2.5137,  4.7148,  ...,  0.1898,  0.4185, -0.2966],\n",
       "          [-0.3762, -0.6851, -0.4727,  ...,  0.3284, -0.5166,  0.7002],\n",
       "          [-0.4841, -0.7422, -0.4375,  ..., -0.4170,  0.3286, -0.0295],\n",
       "          ...,\n",
       "          [-0.0102, -0.3306, -0.3499,  ...,  0.3237,  0.0553,  0.3213],\n",
       "          [-0.0137, -0.3247, -0.3423,  ...,  0.3374, -0.0778,  0.2849],\n",
       "          [-0.0244, -0.2366, -0.2251,  ...,  0.3645, -0.1267,  0.3848]],\n",
       " \n",
       "         [[-3.8926, -2.5137,  4.7148,  ...,  0.1898,  0.4185, -0.2966],\n",
       "          [-0.3762, -0.6851, -0.4727,  ...,  0.3284, -0.5166,  0.7002],\n",
       "          [-0.4841, -0.7422, -0.4375,  ..., -0.4170,  0.3286, -0.0295],\n",
       "          ...,\n",
       "          [-0.0102, -0.3306, -0.3499,  ...,  0.3237,  0.0553,  0.3213],\n",
       "          [-0.0137, -0.3247, -0.3423,  ...,  0.3374, -0.0778,  0.2849],\n",
       "          [-0.0244, -0.2366, -0.2251,  ...,  0.3645, -0.1267,  0.3848]]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]], device='cuda:0',\n",
       "        dtype=torch.float16),\n",
       " tensor([[-0.2942,  0.8794, -0.9185,  ..., -0.6768, -0.3750,  0.6523],\n",
       "         [-0.2942,  0.8794, -0.9185,  ..., -0.6768, -0.3750,  0.6523]],\n",
       "        device='cuda:0', dtype=torch.float16, grad_fn=<ViewBackward0>),\n",
       " tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hentci",
   "language": "python",
   "name": "hentci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
